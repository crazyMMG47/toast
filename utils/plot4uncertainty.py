# Disclaimer: This code is a refactored version of the original uncertainty analysis tools for medical image segmentation models. Generated by Claude AI. 
# uncertainty_analysis.py
"""
Uncertainty analysis tools for medical image segmentation models.
Refactored to follow PhiSeg style with improved axial uncertainty visualization.
"""

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from matplotlib.colors import LinearSegmentedColormap
from scipy.ndimage import binary_erosion, binary_dilation
from typing import Dict, List, Tuple, Optional, Union

from utils.eval_metrics import dice_hard, dice_score_batch, deg, s_ncc


class UncertaintyAnalyzer:
    """Main class for uncertainty analysis following PhiSeg methodology."""
    
    def __init__(self, device: str = 'cuda', n_samples: int = 8):
        self.device = device
        self.n_samples = n_samples
        self.uncertainty_cmap = self._create_uncertainty_colormap()
        
    def _create_uncertainty_colormap(self) -> LinearSegmentedColormap:
        """Create custom colormap for uncertainty visualization."""
        colors = ['#000080', '#0080FF', '#00FF80', '#FFFF00', '#FF8000', '#FF0000']
        return LinearSegmentedColormap.from_list('uncertainty', colors, N=256)
    
    def _dice_distance(self, pred: torch.Tensor, target: torch.Tensor, eps: float = 1e-7) -> torch.Tensor:
        """Compute Dice-based distance for GED calculation."""
        pred_flat = pred.view(-1).float()
        target_flat = target.view(-1).float()
        
        intersection = (pred_flat * target_flat).sum()
        dice_score = (2 * intersection + eps) / (pred_flat.sum() + target_flat.sum() + eps)
        
        return 1 - dice_score
    
    def extract_uncertainty_maps(self, model, data_loader, n_visualize: int = 6) -> Dict:
        """
        Extract uncertainty maps using Monte Carlo sampling.
        
        Args:
            model: Segmentation model with dropout/stochastic layers
            data_loader: DataLoader for evaluation
            n_visualize: Number of samples to store for visualization
            
        Returns:
            Dictionary containing subject and slice-level uncertainty data
        """
        model.eval()
        
        subject_data = []
        slice_data = []
        visualization_samples = {
            'images': [], 'masks': [], 'predictions': [], 'uncertainties': []
        }
        
        print(f"Extracting uncertainty maps with {self.n_samples} MC samples...")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(data_loader):
                images = batch["image"].to(self.device)
                masks = batch["label"].to(self.device)
                batch_size = images.size(0)
                
                # Generate Monte Carlo samples
                mc_predictions = self._generate_mc_samples(model, images)
                
                # Process each subject in batch
                for subj_idx in range(batch_size):
                    subject_results = self._process_subject(
                        images[subj_idx], masks[subj_idx], mc_predictions[:, subj_idx],
                        batch_idx, subj_idx
                    )
                    
                    subject_data.append(subject_results['subject'])
                    slice_data.extend(subject_results['slices'])
                    
                    # Store visualization samples
                    if len(visualization_samples['images']) < n_visualize:
                        self._store_visualization_sample(
                            visualization_samples, subject_results, images[subj_idx], masks[subj_idx]
                        )
        
        return {
            'subjects': subject_data,
            'slices': slice_data,
            'visualization': visualization_samples
        }
    
    def _generate_mc_samples(self, model, images: torch.Tensor) -> torch.Tensor:
        """Generate Monte Carlo samples from stochastic model."""
        predictions = []
        
        for _ in range(self.n_samples):
            # Enable training mode for stochastic sampling
            model.train()
            pred = model(images)
            pred_binary = (torch.sigmoid(pred) > 0.5).float()
            predictions.append(pred_binary)
        
        model.eval()
        return torch.stack(predictions)  # Shape: (n_samples, batch_size, channels, ...)
    
    def _process_subject(self, image: torch.Tensor, mask: torch.Tensor, 
                        mc_preds: torch.Tensor, batch_idx: int, subj_idx: int) -> Dict:
        """Process individual subject to extract uncertainty metrics."""
        
        # Remove channel dimension
        gt = mask[0]  # Shape: (D, H, W) or (H, W)
        samples = mc_preds[:, 0]  # Shape: (n_samples, D, H, W) or (n_samples, H, W)
        img = image[0]
        
        # Compute subject-level metrics
        ged_score = deg(samples, gt, self._dice_distance).item()
        sncc_score = s_ncc(samples[0], gt).item()
        
        # Compute prediction statistics
        pred_mean = samples.mean(0)
        pred_std = samples.std(0)
        
        subject_result = {
            'batch_idx': batch_idx,
            'subject_idx': subj_idx,
            'ged': ged_score,
            'sncc': sncc_score,
            'prediction_mean': pred_mean.cpu().numpy(),
            'prediction_std': pred_std.cpu().numpy(),
            'ground_truth': gt.cpu().numpy(),
            'image': img.cpu().numpy()
        }
        
        # Process slice-level data for 3D volumes
        slice_results = []
        if gt.ndim == 3:  # 3D volume
            slice_results = self._process_volume_slices(
                img, gt, samples, pred_mean, pred_std, batch_idx, subj_idx
            )
        
        return {'subject': subject_result, 'slices': slice_results}
    
    def _process_volume_slices(self, image: torch.Tensor, ground_truth: torch.Tensor,
                              samples: torch.Tensor, pred_mean: torch.Tensor, 
                              pred_std: torch.Tensor, batch_idx: int, subj_idx: int) -> List[Dict]:
        """Process individual slices of 3D volume."""
        slice_results = []
        depth = ground_truth.shape[0]
        
        for slice_idx in range(depth):
            gt_slice = ground_truth[slice_idx]
            
            # Skip empty slices
            if gt_slice.sum() == 0:
                continue
                
            slice_samples = samples[:, slice_idx]
            slice_mean = pred_mean[slice_idx]
            slice_std = pred_std[slice_idx]
            
            # Compute slice-level metrics
            slice_ged = deg(slice_samples, gt_slice, self._dice_distance).item()
            slice_sncc = s_ncc(slice_samples[0], gt_slice).item()
            slice_dice = dice_score_batch(
                slice_mean[None, None], gt_slice[None, None]
            ).item()
            
            # Uncertainty metrics
            mean_uncertainty = slice_std.mean().item()
            max_uncertainty = slice_std.max().item()
            
            # Boundary uncertainty (PhiSeg style)
            boundary_uncertainty = self._compute_boundary_uncertainty(slice_std, gt_slice)
            
            slice_results.append({
                'batch_idx': batch_idx,
                'subject_idx': subj_idx,
                'slice_idx': slice_idx,
                'ged': slice_ged,
                'sncc': slice_sncc,
                'dice': slice_dice,
                'mean_uncertainty': mean_uncertainty,
                'max_uncertainty': max_uncertainty,
                'boundary_uncertainty': boundary_uncertainty,
                'image': image[slice_idx].cpu().numpy(),
                'ground_truth': gt_slice.cpu().numpy(),
                'prediction_mean': slice_mean.cpu().numpy(),
                'prediction_std': slice_std.cpu().numpy()
            })
        
        return slice_results
    
    def _compute_boundary_uncertainty(self, uncertainty_map: torch.Tensor, 
                                    ground_truth: torch.Tensor, boundary_width: int = 3) -> float:
        """Compute uncertainty specifically at object boundaries (PhiSeg style)."""
        gt_np = ground_truth.cpu().numpy()
        
        # Create boundary mask
        eroded = binary_erosion(gt_np, iterations=boundary_width)
        dilated = binary_dilation(gt_np, iterations=boundary_width)
        boundary_mask = dilated & ~eroded
        
        if boundary_mask.sum() == 0:
            return 0.0
        
        uncertainty_np = uncertainty_map.cpu().numpy()
        boundary_uncertainty = uncertainty_np[boundary_mask].mean()
        
        return float(boundary_uncertainty)
    
    def _store_visualization_sample(self, vis_samples: Dict, subject_results: Dict,
                                  image: torch.Tensor, mask: torch.Tensor):
        """Store samples for visualization."""
        vis_samples['images'].append(image[0].cpu().numpy())
        vis_samples['masks'].append(mask[0].cpu().numpy())
        vis_samples['predictions'].append(subject_results['subject']['prediction_mean'])
        vis_samples['uncertainties'].append(subject_results['subject']['prediction_std'])


class UncertaintyVisualizer:
    """Visualization class for uncertainty analysis following PhiSeg style."""
    
    def __init__(self, analyzer: UncertaintyAnalyzer):
        self.analyzer = analyzer
        
    def plot_uncertainty_overview(self, uncertainty_data: Dict, figsize: Tuple[int, int] = (20, 12)):
        """
        Create comprehensive uncertainty overview plot.
        
        Args:
            uncertainty_data: Output from UncertaintyAnalyzer.extract_uncertainty_maps()
            figsize: Figure size
        """
        subjects = uncertainty_data['subjects']
        vis_data = uncertainty_data['visualization']
        
        # Extract metrics
        ged_values = np.array([s['ged'] for s in subjects])
        sncc_values = np.array([s['sncc'] for s in subjects])
        
        # Compute thresholds (PhiSeg style)
        ged_threshold = np.percentile(ged_values, 75)
        sncc_threshold = np.percentile(sncc_values, 25)
        
        # Categorize uncertainty
        categories = self._categorize_uncertainty(ged_values, sncc_values, 
                                                ged_threshold, sncc_threshold)
        
        fig = plt.figure(figsize=figsize)
        gs = fig.add_gridspec(3, 4, height_ratios=[2, 2, 1], width_ratios=[1, 1, 1, 1])
        
        # Main scatter plot
        ax_scatter = fig.add_subplot(gs[0, :2])
        self._plot_uncertainty_scatter(ax_scatter, ged_values, sncc_values, categories,
                                     ged_threshold, sncc_threshold)
        
        # Distribution plots
        ax_ged = fig.add_subplot(gs[0, 2])
        ax_sncc = fig.add_subplot(gs[0, 3])
        self._plot_uncertainty_distributions(ax_ged, ax_sncc, ged_values, sncc_values,
                                           ged_threshold, sncc_threshold)
        
        # Uncertainty regions visualization
        ax_regions = fig.add_subplot(gs[1, :])
        self._plot_axial_uncertainty_regions(ax_regions, vis_data, categories[:len(vis_data['images'])])
        
        # Summary statistics
        ax_stats = fig.add_subplot(gs[2, :])
        self._plot_uncertainty_statistics(ax_stats, ged_values, sncc_values, categories)
        
        plt.tight_layout()
        plt.show()
        
        return {
            'ged_threshold': ged_threshold,
            'sncc_threshold': sncc_threshold,
            'categories': categories
        }
    
    def _categorize_uncertainty(self, ged_values: np.ndarray, sncc_values: np.ndarray,
                              ged_threshold: float, sncc_threshold: float) -> List[str]:
        """Categorize samples into uncertainty levels."""
        categories = []
        for ged, sncc in zip(ged_values, sncc_values):
            if ged > ged_threshold and sncc < sncc_threshold:
                categories.append('High')
            elif ged > ged_threshold or sncc < sncc_threshold:
                categories.append('Medium')
            else:
                categories.append('Low')
        return categories
    
    def _plot_uncertainty_scatter(self, ax, ged_values: np.ndarray, sncc_values: np.ndarray,
                                categories: List[str], ged_threshold: float, sncc_threshold: float):
        """Plot GED vs S-NCC scatter with uncertainty regions."""
        colors = {'Low': '#2E8B57', 'Medium': '#FF8C00', 'High': '#DC143C'}
        
        for category in ['Low', 'Medium', 'High']:
            mask = np.array(categories) == category
            if mask.any():
                ax.scatter(ged_values[mask], sncc_values[mask], 
                         c=colors[category], label=f'{category} Uncertainty',
                         alpha=0.7, s=60, edgecolors='white', linewidth=0.5)
        
        # Add threshold lines
        ax.axvline(ged_threshold, color='red', linestyle='--', alpha=0.7, label='GED Threshold')
        ax.axhline(sncc_threshold, color='blue', linestyle='--', alpha=0.7, label='S-NCC Threshold')
        
        # Styling
        ax.set_xlabel('Generalized Energy Distance (GED)', fontsize=12)
        ax.set_ylabel('Structural Normalized Cross-Correlation (S-NCC)', fontsize=12)
        ax.set_title('Uncertainty Classification (PhiSeg Style)', fontsize=14, fontweight='bold')
        ax.legend(loc='best')
        ax.grid(True, alpha=0.3)
        
        # Add uncertainty region annotations
        self._add_region_annotations(ax, ged_threshold, sncc_threshold)
    
    def _add_region_annotations(self, ax, ged_threshold: float, sncc_threshold: float):
        """Add region annotations to scatter plot."""
        # Get axis limits
        xlim = ax.get_xlim()
        ylim = ax.get_ylim()
        
        # High uncertainty region (top-right)
        high_rect = Rectangle((ged_threshold, sncc_threshold), 
                            xlim[1] - ged_threshold, ylim[1] - sncc_threshold,
                            alpha=0.1, facecolor='red', edgecolor='red', linestyle='--')
        ax.add_patch(high_rect)
        ax.text(ged_threshold + 0.01, ylim[1] - 0.02, 'High\nUncertainty',
               verticalalignment='top', fontsize=10, fontweight='bold', color='red')
        
        # Low uncertainty region (bottom-left)
        low_rect = Rectangle((xlim[0], ylim[0]),
                           ged_threshold - xlim[0], sncc_threshold - ylim[0],
                           alpha=0.1, facecolor='green', edgecolor='green', linestyle='--')
        ax.add_patch(low_rect)
        ax.text(xlim[0] + 0.01, ylim[0] + 0.02, 'Low\nUncertainty',
               verticalalignment='bottom', fontsize=10, fontweight='bold', color='green')
    
    def _plot_uncertainty_distributions(self, ax_ged, ax_sncc, ged_values: np.ndarray,
                                      sncc_values: np.ndarray, ged_threshold: float, sncc_threshold: float):
        """Plot distribution histograms."""
        # GED distribution
        ax_ged.hist(ged_values, bins=30, color='red', alpha=0.7, density=True, edgecolor='black')
        ax_ged.axvline(ged_threshold, color='darkred', linestyle='--', linewidth=2)
        ax_ged.set_title('GED Distribution', fontweight='bold')
        ax_ged.set_xlabel('GED')
        ax_ged.set_ylabel('Density')
        ax_ged.grid(True, alpha=0.3)
        
        # S-NCC distribution
        ax_sncc.hist(sncc_values, bins=30, color='blue', alpha=0.7, density=True, edgecolor='black')
        ax_sncc.axvline(sncc_threshold, color='darkblue', linestyle='--', linewidth=2)
        ax_sncc.set_title('S-NCC Distribution', fontweight='bold')
        ax_sncc.set_xlabel('S-NCC')
        ax_sncc.set_ylabel('Density')
        ax_sncc.grid(True, alpha=0.3)
    
    def _plot_axial_uncertainty_regions(self, ax, vis_data: Dict, categories: List[str]):
        """Plot axial view uncertainty regions (PhiSeg style)."""
        n_samples = min(len(vis_data['images']), 6)
        
        for i in range(n_samples):
            # Get middle slice for 3D volumes
            img = vis_data['images'][i]
            mask = vis_data['masks'][i]
            uncertainty = vis_data['uncertainties'][i]
            
            if img.ndim == 3:  # 3D volume
                mid_slice = img.shape[0] // 2
                img = img[mid_slice]
                mask = mask[mid_slice]
                uncertainty = uncertainty[mid_slice]
            
            # Create subplot
            ax_sub = plt.subplot(1, n_samples, i + 1)
            
            # Display image
            ax_sub.imshow(img, cmap='gray', alpha=0.8)
            
            # Overlay uncertainty map
            uncertainty_masked = np.ma.masked_where(uncertainty < 0.1, uncertainty)
            im = ax_sub.imshow(uncertainty_masked, cmap=self.analyzer.uncertainty_cmap, 
                             alpha=0.7, vmin=0, vmax=np.percentile(uncertainty, 95))
            
            # Add ground truth contour
            ax_sub.contour(mask, levels=[0.5], colors='lime', linewidths=2, alpha=0.9)
            
            # Styling
            category = categories[i] if i < len(categories) else 'Unknown'
            color_map = {'Low': 'green', 'Medium': 'orange', 'High': 'red'}
            title_color = color_map.get(category, 'black')
            
            ax_sub.set_title(f'Sample {i+1}\n{category} Uncertainty', 
                           fontweight='bold', color=title_color)
            ax_sub.axis('off')
            
            # Add colorbar for the last subplot
            if i == n_samples - 1:
                plt.colorbar(im, ax=ax_sub, fraction=0.046, pad=0.04, label='Uncertainty')
        
        ax.axis('off')
        ax.set_title('Axial Uncertainty Regions', fontsize=14, fontweight='bold', pad=20)
    
    def _plot_uncertainty_statistics(self, ax, ged_values: np.ndarray, sncc_values: np.ndarray,
                                   categories: List[str]):
        """Plot summary statistics."""
        # Count categories
        category_counts = {cat: categories.count(cat) for cat in ['Low', 'Medium', 'High']}
        
        # Create statistics text
        stats_text = f"""
        UNCERTAINTY ANALYSIS SUMMARY
        ────────────────────────────
        Total Samples: {len(ged_values)}
        
        GED Statistics:
        • Mean: {ged_values.mean():.3f}
        • Std:  {ged_values.std():.3f}
        • Range: [{ged_values.min():.3f}, {ged_values.max():.3f}]
        
        S-NCC Statistics:
        • Mean: {sncc_values.mean():.3f}
        • Std:  {sncc_values.std():.3f}
        • Range: [{sncc_values.min():.3f}, {sncc_values.max():.3f}]
        
        Uncertainty Categories:
        • Low:    {category_counts.get('Low', 0):2d} samples ({100*category_counts.get('Low', 0)/len(categories):4.1f}%)
        • Medium: {category_counts.get('Medium', 0):2d} samples ({100*category_counts.get('Medium', 0)/len(categories):4.1f}%)
        • High:   {category_counts.get('High', 0):2d} samples ({100*category_counts.get('High', 0)/len(categories):4.1f}%)
        """
        
        ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=10,
               verticalalignment='top', fontfamily='monospace',
               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        ax.axis('off')
    
    def visualize_slice_uncertainty(self, slice_data: List[Dict], n_worst: int = 12, 
                                  figsize: Tuple[int, int] = (20, 16)):
        """
        Visualize worst performing slices with uncertainty overlays.
        
        Args:
            slice_data: List of slice-level uncertainty data
            n_worst: Number of worst slices to display
            figsize: Figure size
        """
        if not slice_data:
            print("No slice-level data available for visualization.")
            return
        
        df = pd.DataFrame(slice_data)
        
        # Compute composite uncertainty score
        df = self._compute_composite_score(df)
        
        # Get worst slices
        worst_slices = df.nlargest(n_worst, 'composite_score')
        
        # Create visualization
        n_cols = 4
        n_rows = int(np.ceil(n_worst / n_cols))
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
        axes = axes.reshape(n_rows, n_cols)
        
        for idx, (_, slice_info) in enumerate(worst_slices.iterrows()):
            row, col = divmod(idx, n_cols)
            ax = axes[row, col]
            
            # Display image
            ax.imshow(slice_info['image'], cmap='gray', alpha=0.8)
            
            # Overlay uncertainty
            uncertainty = slice_info['prediction_std']
            uncertainty_masked = np.ma.masked_where(uncertainty < 0.05, uncertainty)
            im = ax.imshow(uncertainty_masked, cmap=self.analyzer.uncertainty_cmap,
                         alpha=0.7, vmin=0, vmax=np.percentile(uncertainty, 95))
            
            # Add contours
            ax.contour(slice_info['ground_truth'], levels=[0.5], colors='lime', linewidths=2)
            ax.contour(slice_info['prediction_mean'], levels=[0.5], colors='yellow', linewidths=1.5)
            
            # Title with metrics
            title = (f"S{slice_info['subject_idx']} Z{slice_info['slice_idx']}\n"
                    f"Dice: {slice_info['dice']:.3f} | GED: {slice_info['ged']:.3f}")
            ax.set_title(title, fontsize=10, fontweight='bold')
            ax.axis('off')
            
            # Add colorbar to last image in each row
            if col == n_cols - 1:
                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        
        # Hide empty subplots
        for idx in range(len(worst_slices), n_rows * n_cols):
            row, col = divmod(idx, n_cols)
            axes[row, col].axis('off')
        
        plt.suptitle(f'Worst {n_worst} Slices by Composite Uncertainty Score', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        return worst_slices
    
    def _compute_composite_score(self, df: pd.DataFrame) -> pd.DataFrame:
        """Compute composite uncertainty score."""
        # Normalize metrics
        for col in ['ged', 'sncc', 'dice', 'mean_uncertainty', 'boundary_uncertainty']:
            if col in df.columns:
                col_range = df[col].max() - df[col].min()
                if col_range > 0:
                    df[f'norm_{col}'] = (df[col] - df[col].min()) / col_range
                else:
                    df[f'norm_{col}'] = 0
        
        # Compute composite score (higher = worse)
        df['composite_score'] = (
            df.get('norm_ged', 0) +
            (1 - df.get('norm_sncc', 1)) +
            (1 - df.get('norm_dice', 1)) +
            df.get('norm_mean_uncertainty', 0) +
            df.get('norm_boundary_uncertainty', 0)
        ) / 5
        
        return df


def analyze_model_uncertainty(model, loader, device: str = 'cuda', 
                            n_samples: int = 8, n_visualize: int = 6, 
                            n_worst_slices: int = 12, **kwargs) -> Dict:
    """
    Complete uncertainty analysis pipeline following PhiSeg methodology.
    
    Args:
        model: Segmentation model
        loader: Data loader for evaluation (compatible with original parameter name)
        device: Device for computation
        n_samples: Number of Monte Carlo samples
        n_visualize: Number of samples for visualization
        n_worst_slices: Number of worst slices to show
        **kwargs: Additional parameters for backward compatibility
        
    Returns:
        Dictionary containing analysis results
    """
    print("🔍 Starting uncertainty analysis (PhiSeg style)...")
    
    # Initialize analyzer and visualizer
    analyzer = UncertaintyAnalyzer(device=device, n_samples=n_samples)
    visualizer = UncertaintyVisualizer(analyzer)
    
    # Extract uncertainty data
    uncertainty_data = analyzer.extract_uncertainty_maps(model, loader, n_visualize)
    
    # Create overview visualization
    print("📊 Creating uncertainty overview...")
    overview_results = visualizer.plot_uncertainty_overview(uncertainty_data)
    
    # Slice-level analysis if available
    if uncertainty_data['slices']:
        print("🔬 Analyzing slice-level uncertainty...")
        worst_slices = visualizer.visualize_slice_uncertainty(
            uncertainty_data['slices'], n_worst_slices
        )
    else:
        worst_slices = None
        print("ℹ️  No 3D slice data available for detailed analysis.")
    
    print("✅ Uncertainty analysis complete!")
    
    return {
        'uncertainty_data': uncertainty_data,
        'overview_results': overview_results,
        'worst_slices': worst_slices,
        'analyzer': analyzer,
        'visualizer': visualizer
    }